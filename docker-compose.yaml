services:
  spot-ros2:
    build:
      context: .
      dockerfile: Dockerfile.spot-ros2
    volumes:
      - .:/home/spot-teleop
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
    working_dir: /home/spot-teleop/spot-ros2_ws
    runtime: nvidia
    environment:
      - SPOT_NAME=${SPOT_NAME:-Spot}
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - ROS_DOMAIN_ID=8
      - HOME=/home/spot-teleop
      - USER=root
      - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    command: >
      bash -c "
        source /opt/ros/humble/setup.bash &&
        if [ -f /home/spot-teleop/spot-ros2_ws/install/setup.bash ]; then
          echo 'Found existing build, sourcing setup...' &&
          source /home/spot-teleop/spot-ros2_ws/install/setup.bash
        else
          echo 'No existing build found. You can build manually with: colcon build --symlink-install'
        fi &&
        echo 'ðŸŽ‰ Container Spot ROS2 + RealSense pronto! ROS_DOMAIN_ID: 8' &&
        echo 'Para buildar manualmente: colcon build --symlink-install' &&
        exec bash
      "
    stdin_open: true
    tty: true
    network_mode: host
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  zed:
    build:
      context: .
      dockerfile: Dockerfile.zed
    container_name: zed-container
    environment:
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - ROS_DOMAIN_ID=8
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /dev:/dev
      - ./zed_ws:/home/spot-teleop/zed_ws
    working_dir: /home/spot-teleop/zed_ws
    network_mode: host
    ipc: host
    privileged: true
    tty: true
    stdin_open: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      bash -c "
        source /opt/ros/humble/setup.bash &&
        echo 'Setting up rosdep...' &&
        rosdep init || echo 'rosdep already initialized' &&
        rosdep update &&
        echo 'Updating package lists...' &&
        apt-get update &&
        echo 'Installing dependencies...' &&
        rosdep install --from-paths src --ignore-src -r -y --rosdistro humble &&
        echo 'ZED workspace setup...' &&
        if [ -f /home/spot-teleop/zed_ws/install/setup.bash ]; then
          echo 'Found existing build, sourcing setup...' &&
          source /home/spot-teleop/zed_ws/install/setup.bash
        else
          echo 'No existing build found. You can build manually with: colcon build --symlink-install'
        fi &&
        echo 'ðŸŽ¥ Container ZED + NVBlox pronto! ROS_DOMAIN_ID: 8' &&
        echo 'Para buildar manualmente: colcon build --symlink-install' &&
        exec bash
      "

  isaac-sim:
    build:
      context: .
      dockerfile: Dockerfile.isaac
    runtime: nvidia
    network_mode: host
    ipc: host
    privileged: true
    environment:
      - ACCEPT_EULA=Y
      - PRIVACY_CONSENT=Y
      - ROS_DOMAIN_ID=8
      - RMW_IMPLEMENTATION=rmw_fastrtps_cpp
      # GUI (comente se for headless)
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - __NGX_CONF_FILE=/usr/share/nvidia/nvidia-ngx-conf.json
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:rw  # GUI
      - ./isaac-sim_ws:/workspace:rw  # ðŸŽ¯ Workspace persistente montado em /workspace
      # caches (pra carregar + rÃ¡pido)
      - ./isaac-sim_ws/cache/kit:/isaac-sim/kit/cache:rw
      - ./isaac-sim_ws/cache/ov:/root/.cache/ov:rw
      - ./isaac-sim_ws/cache/pip:/root/.cache/pip:rw
      - ./isaac-sim_ws/cache/glcache:/root/.cache/nvidia/GLCache:rw
      - ./isaac-sim_ws/cache/computecache:/root/.nv/ComputeCache:rw

      # ðŸ”¥ NGX injection
      - /usr/bin/nvidia-ngx-updater:/usr/bin/nvidia-ngx-updater:ro
      - /lib/x86_64-linux-gnu/libnvidia-ngx.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ngx.so.1:ro
      - /usr/share/nvidia/ngx:/usr/share/nvidia/ngx
      - /usr/share/nvidia/nvidia-ngx-conf.json:/usr/share/nvidia/nvidia-ngx-conf.json:ro
      
      # ForÃ§a as libs de vÃ­deo do host dentro do container:
      - /lib/x86_64-linux-gnu/libnvcuvid.so.1:/usr/lib/x86_64-linux-gnu/libnvcuvid.so.1:ro
      - /lib/x86_64-linux-gnu/libnvcuvid.so:/usr/lib/x86_64-linux-gnu/libnvcuvid.so:ro
      - /lib/x86_64-linux-gnu/libnvidia-encode.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-encode.so.1:ro
      - /lib/x86_64-linux-gnu/libnvidia-encode.so:/usr/lib/x86_64-linux-gnu/libnvidia-encode.so:ro
    stdin_open: true
    tty: true
    entrypoint: ["/bin/bash", "-lc", "sleep infinity"]  # NÃ£o iniciar Isaac automaticamente
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, graphics, utility]

